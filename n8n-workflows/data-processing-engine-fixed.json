{
  "name": "Data Processing Engine - Bulk Processing",
  "nodes": [
    {
      "parameters": {
        "path": "bulk-process",
        "options": {
          "rawBody": true
        }
      },
      "id": "bulk-webhook",
      "name": "Receive Bulk Data Request",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1,
      "position": [200, 300],
      "webhookId": "bulk-data-webhook"
    },
    {
      "parameters": {
        "jsCode": "// Intelligent data analysis and preprocessing\nconst inputData = $input.all();\n\nif (!inputData || inputData.length === 0) {\n  throw new Error('No data received');\n}\n\nlet requestData;\ntry {\n  const bodyData = inputData[0].json.body;\n  if (typeof bodyData === 'string') {\n    requestData = JSON.parse(bodyData);\n  } else {\n    requestData = bodyData;\n  }\n} catch (error) {\n  throw new Error('Invalid JSON format in request body');\n}\n\nconst students = requestData.students || [];\nconst options = requestData.options || {};\n\nif (!Array.isArray(students) || students.length === 0) {\n  throw new Error('Student data format error or empty');\n}\n\nconsole.log('Received ' + students.length + ' student records');\n\n// Intelligent analysis of data scale, determine processing strategy\nconst totalStudents = students.length;\nlet processingStrategy = 'single'; // single, parallel, distributed\nlet pipelineCount = 1;\nlet chunkSize = 50;\n\nif (totalStudents > 1000) {\n  processingStrategy = 'distributed';\n  pipelineCount = Math.min(Math.ceil(totalStudents / 1500), 5); // Max 5 pipelines\n  chunkSize = Math.min(Math.ceil(totalStudents / (pipelineCount * 20)), 100); // Dynamic batch size\n} else if (totalStudents > 200) {\n  processingStrategy = 'parallel';\n  pipelineCount = Math.min(Math.ceil(totalStudents / 500), 3); // Max 3 pipelines\n  chunkSize = 50;\n}\n\n// Validate each record\nconst validatedData = [];\nconst invalidData = [];\n\nstudents.forEach((student, index) => {\n  const validation = {\n    rowIndex: index + 1,\n    email: student.email,\n    courseId: student.courseId || student.course_id,\n    courseName: student.courseName || student.course_name || 'Course-' + (student.courseId || student.course_id),\n    className: student.className || student.class_name,\n    isValid: true,\n    errors: []\n  };\n  \n  // Email validation\n  const emailRegex = /^[^\\\\s@]+@[^\\\\s@]+\\\\.[^\\\\s@]+$/;\n  if (!validation.email || !emailRegex.test(validation.email)) {\n    validation.isValid = false;\n    validation.errors.push('Invalid email format');\n  }\n  \n  // Course ID validation\n  if (!validation.courseId) {\n    validation.isValid = false;\n    validation.errors.push('Missing course ID');\n  }\n  \n  if (validation.isValid) {\n    validatedData.push({\n      email: validation.email,\n      courseId: validation.courseId,\n      courseName: validation.courseName,\n      className: validation.className,\n      originalIndex: index,\n      assignedPipeline: validatedData.length % pipelineCount // Assign to different pipelines\n    });\n  } else {\n    invalidData.push(validation);\n  }\n});\n\n// Split data into different processing pipelines\nconst pipelines = Array.from({length: pipelineCount}, () => []);\nvalidatedData.forEach(student => {\n  pipelines[student.assignedPipeline].push(student);\n});\n\nconst processingPlan = {\n  batchId: 'bulk_' + Date.now(),\n  strategy: processingStrategy,\n  totalStudents: totalStudents,\n  validStudents: validatedData.length,\n  invalidStudents: invalidData.length,\n  pipelineCount: pipelineCount,\n  chunkSize: chunkSize,\n  estimatedTime: Math.ceil(validatedData.length * 0.8 / 60), // seconds to minutes\n  pipelines: pipelines.map((pipeline, index) => ({\n    pipelineId: 'pipeline_' + (index + 1),\n    studentCount: pipeline.length,\n    students: pipeline\n  })),\n  invalidData: invalidData,\n  startTime: new Date().toISOString(),\n  options: options\n};\n\nconsole.log('Processing strategy: ' + processingStrategy + ', pipelines: ' + pipelineCount + ', estimated time: ' + processingPlan.estimatedTime + ' minutes');\n\nreturn [processingPlan];"
      },
      "id": "analyze-data",
      "name": "Analyze Data Intelligence",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [400, 300]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "leftValue": "={{$json.validStudents}}",
            "operation": "larger",
            "rightValue": 0
          }
        }
      },
      "id": "check-valid-data",
      "name": "Has Valid Data?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [600, 300]
    },
    {
      "parameters": {
        "jsCode": "// Distribute data to different processing pipelines\nconst processingPlan = $input.first().json;\n\n// Create independent processing items for each pipeline\nconst pipelineItems = processingPlan.pipelines.map((pipeline, index) => ({\n  pipelineId: pipeline.pipelineId,\n  pipelineIndex: index,\n  students: pipeline.students,\n  studentCount: pipeline.studentCount,\n  batchId: processingPlan.batchId,\n  chunkSize: processingPlan.chunkSize,\n  totalPipelines: processingPlan.pipelineCount,\n  startTime: processingPlan.startTime\n}));\n\nconsole.log('Distributing data to ' + pipelineItems.length + ' processing pipelines');\n\nreturn pipelineItems;"
      },
      "id": "distribute-data",
      "name": "Distribute to Pipelines",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [800, 220]
    },
    {
      "parameters": {
        "workflowId": "student-batch-management",
        "options": {
          "loadFromCurrentWorkflow": false,
          "waitForExecution": true
        }
      },
      "id": "pipeline-1",
      "name": "Processing Pipeline 1",
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1,
      "position": [1000, 120]
    },
    {
      "parameters": {
        "workflowId": "student-batch-management",
        "options": {
          "loadFromCurrentWorkflow": false,
          "waitForExecution": true
        }
      },
      "id": "pipeline-2",
      "name": "Processing Pipeline 2",
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1,
      "position": [1000, 220]
    },
    {
      "parameters": {
        "workflowId": "student-batch-management",
        "options": {
          "loadFromCurrentWorkflow": false,
          "waitForExecution": true
        }
      },
      "id": "pipeline-3",
      "name": "Processing Pipeline 3",
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1,
      "position": [1000, 320]
    },
    {
      "parameters": {
        "mode": "combine",
        "combinationMode": "mergeByIndex",
        "options": {}
      },
      "id": "merge-pipelines",
      "name": "Merge Pipeline Results",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 2.1,
      "position": [1200, 220]
    },
    {
      "parameters": {
        "jsCode": "// Generate comprehensive processing report\nconst pipelineResults = $input.all();\nconst originalPlan = $('analyze-data').first().json;\n\n// Aggregate results from all pipelines\nlet totalProcessed = 0;\nlet totalSuccessful = 0;\nlet totalAlreadyExists = 0;\nlet totalFailed = 0;\nconst allDetails = [];\nconst pipelineReports = [];\n\npipelineResults.forEach((result, index) => {\n  const pipelineData = result.json;\n  \n  totalProcessed += pipelineData.totalProcessed || 0;\n  totalSuccessful += pipelineData.successful || 0;\n  totalAlreadyExists += pipelineData.alreadyExists || 0;\n  totalFailed += pipelineData.failed || 0;\n  \n  if (pipelineData.details) {\n    allDetails.push(...pipelineData.details);\n  }\n  \n  pipelineReports.push({\n    pipelineId: 'pipeline_' + (index + 1),\n    processed: pipelineData.totalProcessed || 0,\n    successful: pipelineData.successful || 0,\n    alreadyExists: pipelineData.alreadyExists || 0,\n    failed: pipelineData.failed || 0,\n    successRate: pipelineData.successRate || 0\n  });\n});\n\nconst endTime = new Date();\nconst startTime = new Date(originalPlan.startTime);\nconst processingTimeSeconds = Math.round((endTime - startTime) / 1000);\nconst processingTimeMinutes = Math.round(processingTimeSeconds / 60 * 100) / 100;\n\nconst overallSuccessRate = totalProcessed > 0 ? \n  Math.round(((totalSuccessful + totalAlreadyExists) / totalProcessed) * 100) : 0;\n\n// Analyze error patterns\nconst errorAnalysis = {};\nallDetails.forEach(detail => {\n  if (detail.status !== 'success' && detail.status !== 'already_exists') {\n    const errorType = detail.status;\n    if (!errorAnalysis[errorType]) {\n      errorAnalysis[errorType] = 0;\n    }\n    errorAnalysis[errorType]++;\n  }\n});\n\n// Calculate processing efficiency\nconst studentsPerSecond = processingTimeSeconds > 0 ? \n  Math.round(totalProcessed / processingTimeSeconds * 100) / 100 : 0;\n\nconst comprehensiveReport = {\n  batchId: originalPlan.batchId,\n  \n  // Timing statistics\n  timing: {\n    startTime: originalPlan.startTime,\n    endTime: endTime.toISOString(),\n    processingTimeSeconds: processingTimeSeconds,\n    processingTimeMinutes: processingTimeMinutes,\n    estimatedTime: originalPlan.estimatedTime,\n    efficiency: originalPlan.estimatedTime > 0 ? \n      Math.round((originalPlan.estimatedTime / processingTimeMinutes) * 100) : 0\n  },\n  \n  // Processing statistics\n  statistics: {\n    originalTotal: originalPlan.totalStudents,\n    validForProcessing: originalPlan.validStudents,\n    invalidData: originalPlan.invalidStudents,\n    totalProcessed: totalProcessed,\n    successful: totalSuccessful,\n    alreadyExists: totalAlreadyExists,\n    failed: totalFailed,\n    successRate: overallSuccessRate,\n    studentsPerSecond: studentsPerSecond\n  },\n  \n  // Pipeline performance\n  pipelinePerformance: {\n    strategy: originalPlan.strategy,\n    pipelineCount: originalPlan.pipelineCount,\n    chunkSize: originalPlan.chunkSize,\n    reports: pipelineReports\n  },\n  \n  // Error analysis\n  errorAnalysis: errorAnalysis,\n  \n  // Detailed results\n  detailedResults: allDetails\n};\n\n// Generate human-readable report\nlet reportMessage = 'Bulk Data Processing Complete Report\\\\n\\\\n';\nreportMessage += 'Batch ID: ' + comprehensiveReport.batchId + '\\\\n\\\\n';\nreportMessage += 'Processing Time Statistics:\\\\n';\nreportMessage += '• Start Time: ' + new Date(comprehensiveReport.timing.startTime).toLocaleString() + '\\\\n';\nreportMessage += '• End Time: ' + new Date(comprehensiveReport.timing.endTime).toLocaleString() + '\\\\n';\nreportMessage += '• Actual Processing Time: ' + comprehensiveReport.timing.processingTimeMinutes + ' minutes\\\\n';\nreportMessage += '• Estimated Time: ' + originalPlan.estimatedTime + ' minutes\\\\n';\nreportMessage += '• Efficiency Score: ' + comprehensiveReport.timing.efficiency + '%\\\\n';\nreportMessage += '• Processing Speed: ' + comprehensiveReport.statistics.studentsPerSecond + ' students/second\\\\n\\\\n';\n\nreportMessage += 'Processing Statistics:\\\\n';\nreportMessage += '• Original Total: ' + comprehensiveReport.statistics.originalTotal + '\\\\n';\nreportMessage += '• Valid Data: ' + comprehensiveReport.statistics.validForProcessing + '\\\\n';\nreportMessage += '• Invalid Data: ' + comprehensiveReport.statistics.invalidData + '\\\\n';\nreportMessage += '• Actually Processed: ' + comprehensiveReport.statistics.totalProcessed + '\\\\n';\nreportMessage += '• Successfully Added: ' + comprehensiveReport.statistics.successful + '\\\\n';\nreportMessage += '• Already Exists: ' + comprehensiveReport.statistics.alreadyExists + '\\\\n';\nreportMessage += '• Failed: ' + comprehensiveReport.statistics.failed + '\\\\n';\nreportMessage += '• Overall Success Rate: ' + comprehensiveReport.statistics.successRate + '%\\\\n\\\\n';\n\nreportMessage += 'Processing Strategy:\\\\n';\nreportMessage += '• Strategy Type: ' + comprehensiveReport.pipelinePerformance.strategy + '\\\\n';\nreportMessage += '• Parallel Pipelines: ' + comprehensiveReport.pipelinePerformance.pipelineCount + '\\\\n';\nreportMessage += '• Batch Size: ' + comprehensiveReport.pipelinePerformance.chunkSize + ' records/batch\\\\n\\\\n';\n\nreportMessage += 'Pipeline Performance:\\\\n';\ncomprehensiveReport.pipelinePerformance.reports.forEach(pipeline => {\n  reportMessage += '• ' + pipeline.pipelineId + ': ' + pipeline.processed + ' records (Success Rate: ' + pipeline.successRate + '%)\\\\n';\n});\n\nif (Object.keys(errorAnalysis).length > 0) {\n  reportMessage += '\\\\nError Analysis:\\\\n';\n  Object.entries(errorAnalysis).forEach(([error, count]) => {\n    reportMessage += '• ' + error + ': ' + count + ' records\\\\n';\n  });\n}\n\ncomprehensiveReport.reportMessage = reportMessage;\n\nconsole.log(reportMessage);\n\nreturn [comprehensiveReport];"
      },
      "id": "comprehensive-report",
      "name": "Generate Comprehensive Report",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1400, 220]
    },
    {
      "parameters": {
        "authentication": "none",
        "httpMethod": "POST",
        "url": "=http://localhost:3000/api/progress",
        "options": {
          "bodyContentType": "json",
          "jsonBody": "={{JSON.stringify({\n  \"type\": \"bulk_processing_complete\",\n  \"batchId\": $json.batchId,\n  \"report\": $json,\n  \"timestamp\": $json.timing.endTime\n})}}",
          "timeout": 10000
        }
      },
      "id": "notify-completion",
      "name": "Notify Processing Complete",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [1600, 220],
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "httpMethod": "POST",
        "url": "=http://localhost:3000/api/error",
        "options": {
          "bodyContentType": "json",
          "jsonBody": "={{JSON.stringify({\n  \"error\": \"No valid data to process\",\n  \"batchId\": $node['analyze-data'].json.batchId,\n  \"details\": $node['analyze-data'].json,\n  \"timestamp\": new Date().toISOString()\n})}}",
          "timeout": 10000
        }
      },
      "id": "notify-error",
      "name": "Notify Processing Error",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [800, 380],
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "options": {}
      },
      "id": "end-workflow",
      "name": "End Workflow",
      "type": "n8n-nodes-base.noOp",
      "typeVersion": 1,
      "position": [1800, 300]
    }
  ],
  "connections": {
    "Receive Bulk Data Request": {
      "main": [
        [
          {
            "node": "Analyze Data Intelligence",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Analyze Data Intelligence": {
      "main": [
        [
          {
            "node": "Has Valid Data?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Has Valid Data?": {
      "main": [
        [
          {
            "node": "Distribute to Pipelines",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Notify Processing Error",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Distribute to Pipelines": {
      "main": [
        [
          {
            "node": "Processing Pipeline 1",
            "type": "main",
            "index": 0
          },
          {
            "node": "Processing Pipeline 2",
            "type": "main",
            "index": 0
          },
          {
            "node": "Processing Pipeline 3",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Processing Pipeline 1": {
      "main": [
        [
          {
            "node": "Merge Pipeline Results",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Processing Pipeline 2": {
      "main": [
        [
          {
            "node": "Merge Pipeline Results",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Processing Pipeline 3": {
      "main": [
        [
          {
            "node": "Merge Pipeline Results",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge Pipeline Results": {
      "main": [
        [
          {
            "node": "Generate Comprehensive Report",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Generate Comprehensive Report": {
      "main": [
        [
          {
            "node": "Notify Processing Complete",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Notify Processing Complete": {
      "main": [
        [
          {
            "node": "End Workflow",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Notify Processing Error": {
      "main": [
        [
          {
            "node": "End Workflow",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1",
    "saveManualExecutions": true,
    "callerPolicy": "workflowsFromSameOwner",
    "errorWorkflow": "error-handler"
  },
  "staticData": {},
  "tags": [
    {
      "createdAt": "2025-01-23T00:00:00.000Z",
      "updatedAt": "2025-01-23T00:00:00.000Z",
      "id": "bulk-processing",
      "name": "Bulk Processing"
    },
    {
      "createdAt": "2025-01-23T00:00:00.000Z",
      "updatedAt": "2025-01-23T00:00:00.000Z",
      "id": "distributed",
      "name": "Distributed Processing"
    }
  ],
  "meta": {
    "templateCredsSetupCompleted": true
  },
  "id": "data-processing-engine",
  "versionId": "1.0.0"
}